# Gestor de Requisitos â€“ Backend

Backend del **Gestor de Requisitos**, una aplicaciÃ³n para capturar, analizar y mejorar requisitos de software con asistencia de IA.  
Desarrollado en **FastAPI** + **SQLModel** + **PostgreSQL**, con integraciÃ³n local con modelos **Ollama**.

---

## ğŸš€ CaracterÃ­sticas

- **AutenticaciÃ³n JWT** con registro/login de usuarios.
- **GestiÃ³n de proyectos** con CRUD completo.
- **Chat interactivo con IA** para:
  - Captura inicial de descripciÃ³n del proyecto.
  - Preguntas aclaratorias.
  - GeneraciÃ³n inicial de requisitos.
  - AnÃ¡lisis y mejora iterativa de requisitos.
  - Modo de conversaciÃ³n libre (tipo ChatGPT).
- **MÃ¡quina de estados** (`StateMachine`) para orquestar el flujo:
  - `init` â†’ `software_questions` â†’ `new_requisites` â†’ `stall`
  - Ciclo iterativo `stall` â†” `analyze_requisites`.
- **GestiÃ³n de requisitos** con soporte de categorÃ­as, prioridades y estados.
- **Soporte multi-idioma** para interacciÃ³n con la IA.
- **Carga de archivos de ejemplo** (usados solo en generaciÃ³n/anÃ¡lisis de requisitos).

---

## ğŸ“‚ Estructura principal

app/
â”œâ”€â”€ api/
â”‚ â””â”€â”€ endpoints/
â”‚ â”œâ”€â”€ auth.py # Registro/Login de usuarios
â”‚ â”œâ”€â”€ projects.py # CRUD de proyectos
â”‚ â”œâ”€â”€ chat_message.py # LÃ³gica de chat y mÃ¡quina de estados
â”‚ â””â”€â”€ state_machine.py # Control de cambios de estado
â”‚
â”œâ”€â”€ models/ # Modelos SQLModel (User, Project, Requirement, StateMachine, ChatMessage)
â”œâ”€â”€ schemas/ # Schemas Pydantic para validaciÃ³n de entrada/salida
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ ollama_client.py # Cliente para llamar al modelo local Ollama
â”‚ â”œâ”€â”€ prompt_loader.py # Carga de plantillas de prompt
â”‚ â”œâ”€â”€ message_loader.py # Carga de mensajes predefinidos
â”‚ â””â”€â”€ analyze_parser.py # Parser de salida de IA en anÃ¡lisis de requisitos
â”‚
â”œâ”€â”€ database.py # ConfiguraciÃ³n y conexiÃ³n a PostgreSQL
â””â”€â”€ main.py # Punto de entrada FastAPI


---

## ğŸ›  Requisitos previos

- **Python** 3.11+
- **PostgreSQL**
- **Ollama** instalado y ejecutando el modelo configurado (por defecto `llama3:8b`)
- **Poetry** o `pip` para gestionar dependencias

---

## âš™ï¸ InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/tu-org/gestor-requisitos-backend.git
cd gestor-requisitos-backend

# Crear entorno virtual e instalar dependencias
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Variables de entorno (ejemplo)
export DATABASE_URL=postgresql://usuario:password@localhost:5432/gestorrequisitos
export SECRET_KEY=clave_secreta
# URL base de Ollama (opcional, por defecto http://localhost:11434)
export OLLAMA_URL=http://localhost:11434
# Log SQL detallado (opcional, por defecto deshabilitado)
export SQL_ECHO=true

# Crear tablas
python3 -m create_tables

# Ejecutar backend
uvicorn app.main:app --reload
```

# Flujo de trabajo
## Estados de StateMachine
**init** â€“ El sistema solicita al usuario una descripciÃ³n inicial de su proyecto.

**software_questions** â€“ La IA formula preguntas aclaratorias basadas en esa descripciÃ³n.

**new_requisites** â€“ La IA genera un listado inicial de requisitos en formato estructurado.

**stall** â€“ EdiciÃ³n libre de requisitos y conversaciÃ³n libre con la IA.

**analyze_requisites** â€“ La IA analiza la lista actual de requisitos y formula nuevas preguntas para mejorarlos.

init â†’ software_questions â†’ new_requisites â†’ stall
        â†‘                                      â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ analyze_requisites â”€â”€â”€â”˜

# Endpoints Principales

| MÃ©todo | Ruta                          | DescripciÃ³n                   |
| ------ | ----------------------------- | ----------------------------- |
| POST   | `/auth/register`              | Registro de usuario           |
| POST   | `/auth/login`                 | Login de usuario              |
| GET    | `/auth/me`                    | Datos del usuario autenticado |
| GET    | `/projects`                   | Listar proyectos              |
| POST   | `/projects`                   | Crear proyecto                |
| GET    | `/chat_messages/project/{id}` | Mensajes de un proyecto       |
| POST   | `/chat_messages`              | Enviar mensaje (IA o usuario) |
| GET    | `/state_machine/project/{id}` | Estado actual                 |
| POST   | `/state_machine/project/{id}` | Cambiar estado                |


# IntegraciÃ³n con Ollama
## GeneraciÃ³n de prompts:
Los prompts estÃ¡n en **static/prompts/*.txt** y se formatean con **prompt_loader.py**.

## Modelos soportados:
Por defecto llama3:8b pero configurable vÃ­a OLLAMA_MODEL.

La URL base del servicio Ollama se configura mediante `OLLAMA_URL` o el
atributo `ollama_url` en `Settings`; si no se especifica, se usarÃ¡
`http://localhost:11434`.

## Modo stall:
Los mensajes se envÃ­an sin prompt fijo; el backend compone contexto con la conversaciÃ³n previa y requisitos actuales.

# ğŸ“Œ Notas importantes
Los archivos de ejemplo no se usan en el modo stall salvo que el usuario lo indique explÃ­citamente.

El idioma de interacciÃ³n se guarda en StateMachine.extra["lang"] y se fuerza en todos los prompts.

El token JWT expira; es recomendable implementar refresh tokens en el frontend para evitar redirecciones a login.

# ğŸ“œ Licencia
MIT â€“ Uso libre con atribuciÃ³n.